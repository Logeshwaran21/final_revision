data = [("James", "Sales", 3000), \

    ("Michael", "Sales", 4600), \

    ("Robert", "Sales", 4100), \

    ("Maria", "Finance", 3000), \

    ("James", "Sales", 3000), \

    ("Scott", "Finance", 3300), \

    ("Jen", "Finance", 3900), \

    ("Jeff", "Marketing", 3000), \

    ("Kumar", "Marketing", 2000), \

    ("Saif", "Sales", 4100) \

  ]

column= ["employee_name", "department", "salary"]

df = spark.createDataFrame(data = data, schema = column)

#distinct rows
df1= df.distinct().show()
#drop duplicates
df.show()
df2 = df.dropDuplicates()
df2.show()

#dropduplicates on specific column
df.show()
df3 = df.dropDuplicates(["salary"])
df3.show()
df4 = df.dropDuplicates(["department"])
df4.display()

#user defined function to change the col_name
#col_rename = df.withColumnRenamed("old_column","newcolumn")
def col_rename(df):
    for col in df.columns:
        df = df.withColumnRenamed(col, col.upper())
    return df


#UDF column renamed
renamed_upper = col_rename(df)

renamed_upper.display()
#change department values into uppercase
from pyspark.sql.functions import *
renamed_upper = renamed_upper.withColumn('DEPARTMENT', upper(renamed_upper['DEPARTMENT']))

renamed_upper.display()
